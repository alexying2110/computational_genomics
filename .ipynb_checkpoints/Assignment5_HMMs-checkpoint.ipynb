{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Assignment 5- HMMs\n",
    "In this assignment, you will be implementing the dynamic programs for forward and backward algorithms. Feel free to modify the skeleton code as you deem necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM:\n",
    "    def __init__(self, states, symbols, trans_probs, emission_probs, start_distribution = None):\n",
    "        self.states = states \n",
    "        self.n_states = len(self.states) \n",
    "        self.transition_matrix = trans_probs\n",
    "        self.emission_matrix = emission_probs\n",
    "        self.symbols = symbols\n",
    "        self.n_symbols = len(self.symbols)\n",
    "        if start_distribution is None:\n",
    "            self.start_distribution = np.ones(self.n_states)/self.n_states\n",
    "\n",
    "\n",
    "class log_probs_2D:\n",
    "    \"\"\"\n",
    "    This class allows us to compute the probability of each observation under each state.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_states, n_times):\n",
    "        # n_times is the length of the sequence you have.\n",
    "        self.logprobs = np.zeros(shape=[n_times + 1, n_states])\n",
    "    \n",
    "    def get_vec(self, n):\n",
    "        # Retrieves a row of probabilities from logprobs\n",
    "        assert n <= np.shape(self.logprobs)[0]\n",
    "        return self.logprobs[n,:]\n",
    "\n",
    "    def set_vec(self, n, new_log_probs):\n",
    "        # Sets a row of probabilities in logprobs\n",
    "        assert n <= np.shape(self.logprobs)[0]\n",
    "        self.logprobs[n,:] = new_log_probs\n",
    "        return self.logprobs[n,:]\n",
    "\n",
    "\n",
    "class iter_algorithm:\n",
    "    \"\"\"\n",
    "    The parent class which defines general approach for running forward backward\n",
    "    Later, we extend this class by defining methods init_DP and iter_DP specific to forward/backward methods\n",
    "    and uses your implementation of init_DP and iter_DP.\n",
    "    \"\"\"\n",
    "    def __init__(self, hmm, observations, init_index, iteration_range, is_forward):\n",
    "        self.lprobs = log_probs_2D(n_states = hmm.n_states, n_times = len(observations))\n",
    "        self.lprobs.set_vec(init_index, self.init_DP(hmm))\n",
    "        for n in iteration_range:\n",
    "            if (is_forward):\n",
    "                current = n+1\n",
    "                prev = n\n",
    "            else:\n",
    "                current = n\n",
    "                prev = n+1\n",
    "            self.lprobs.set_vec(current, self.iter_DP(self.lprobs.get_vec(prev), hmm, observations[n]))\n",
    "\n",
    "    \n",
    "class forward(iter_algorithm):\n",
    "    \"\"\"\n",
    "    forward inherits from iter_algorithm. \n",
    "    \"\"\"\n",
    "    def __init__(self, hmm, observations):\n",
    "        iter_algorithm.__init__(self, hmm, observations, 0, range(len(observations)), True)\n",
    "\n",
    "    def init_DP(self, hmm):\n",
    "        return np.full((hmm.n_states), -np.log(hmm.n_states))\n",
    "    \n",
    "    def iter_DP(self, prev_log_probs, hmm, observation):\n",
    "        ob_idx = hmm.symbols.index(observation)\n",
    "        log_probs = np.full((hmm.n_states), np.NINF)\n",
    "        for s in range(len(log_probs)):\n",
    "            for p in range(len(prev_log_probs)):\n",
    "                state_prob = prev_log_probs[p] + np.log(hmm.transition_matrix[p, s]) + np.log(hmm.emission_matrix[s, ob_idx])\n",
    "                log_probs[s] = np.logaddexp(log_probs[s], state_prob)\n",
    "        return log_probs\n",
    "\n",
    "    def overall_loglikelihood(self):\n",
    "        last_probs = self.lprobs.logprobs[-1,:]\n",
    "        overall_prob = np.NINF\n",
    "        for p in last_probs:\n",
    "            overall_prob = np.logaddexp(overall_prob, p)\n",
    "        return overall_prob\n",
    "\n",
    "class backward(iter_algorithm):\n",
    "    def __init__(self, hmm, observations):\n",
    "        iter_algorithm.__init__(self, hmm, observations, len(observations), reversed(range(len(observations))), False)                     \n",
    "\n",
    "    def init_DP(self, hmm):\n",
    "        return np.zeros((hmm.n_states))\n",
    "                               \n",
    "    def iter_DP(self, prev_log_probs, hmm, observation):\n",
    "        ob_idx = hmm.symbols.index(observation)\n",
    "        log_probs = np.full((hmm.n_states), np.NINF)\n",
    "        for s in range(len(log_probs)):\n",
    "            for p in range(len(prev_log_probs)):\n",
    "                state_prob = prev_log_probs[p] + np.log(hmm.transition_matrix[s, p]) + np.log(hmm.emission_matrix[p, ob_idx])\n",
    "                log_probs[s] = np.logaddexp(log_probs[s], state_prob)\n",
    "        return log_probs\n",
    "    \n",
    "    def overall_loglikelihood(self):\n",
    "        last_probs = self.lprobs.logprobs[0,:]\n",
    "        overall_prob = np.NINF\n",
    "        for p in last_probs:\n",
    "            overall_prob = np.logaddexp(overall_prob, p)\n",
    "        return overall_prob\n",
    "        \n",
    "class HMM_plus_data:\n",
    "    def __init__(self, hmm, observations):\n",
    "        self.observations = observations\n",
    "        self.HMM = hmm\n",
    "        self.fwd = forward(hmm, observations)\n",
    "        self.bwd = backward(hmm, observations)\n",
    "                                     \n",
    "    def compute_logprob_joint(self, n):\n",
    "        ## returns a matrix (np.ndarray) of log probalities whose k, kprime entry lists the log probability\n",
    "        ## of the HMM being at a state k at time n,and at state kprime at time n+1 \n",
    "        \n",
    "        '''\n",
    "        P(p(n) = k, p(n + 1) = k', Y) = P(f_k(n) * T_{k k'} * E_{k' y(n+1)} * b_k'(n+1))\n",
    "        '''\n",
    "        logprob_joint = np.zeros((self.HMM.n_states, self.HMM.n_states))\n",
    "        ob_idx = hmm.symbols.index(self.observations[n])\n",
    "        for k in range(self.HMM.n_states):\n",
    "            for kprime in range(self.HMM.n_states):\n",
    "                logprob_joint[k, kprime] = \\\n",
    "                    self.fwd.lprobs.get_vec(n)[k] + \\\n",
    "                    np.log(self.HMM.transition_matrix[k, kprime]) + \\\n",
    "                    np.log(self.HMM.emission_matrix[kprime, ob_idx]) + \\\n",
    "                    self.bwd.lprobs.get_vec(n + 1)[kprime]\n",
    "                    \n",
    "        return logprob_joint\n",
    "                      \n",
    "    def compute_prob_conditional(self, n):\n",
    "        ## returns a matrix (np.ndarray) of log probabilities whose k, kprime entry lists the log probability \n",
    "        ## of the HMM being at state kprime at time n+1 given at a state k at time n\n",
    "        '''\n",
    "        P(p(n+1) = k' | p(n) = k, Y) = P(p(n+1) = k', p(n) = k, Y) / P(p(n) = k, Y)\n",
    "        P(p(n) = k, Y) = f_k(n) * b_k(n)\n",
    "        '''\n",
    "        logprob_cond = self.compute_logprob_joint(n)\n",
    "        for k in range(self.HMM.n_states):\n",
    "            for kprime in range(self.HMM.n_states):\n",
    "                logprob_cond[k, kprime] = \\\n",
    "                    logprob_cond[k, kprime] - \\\n",
    "                    self.fwd.lprobs.get_vec(n)[k] - \\\n",
    "                    self.bwd.lprobs.get_vec(n)[k]\n",
    "        return logprob_cond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on https://en.wikipedia.org/wiki/Forward%E2%80%93backward_algorithm#Example\n",
    "\n",
    "#State 0 is rain\n",
    "#State 1 is no rain \n",
    "states = [0,1]\n",
    "\n",
    "# 3 is umbrella, 4 is no umbrella\n",
    "symbols = ['3','4']\n",
    "trans_probs = np.array([[0.7, 0.3],[0.3, 0.7]])\n",
    "emission_probs = np.array([[0.9, 0.1],[0.2,0.8]])\n",
    "hmm = HMM(states, symbols, trans_probs, emission_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.69314718 -0.69314718]\n",
      " [-0.7985077  -2.30258509]\n",
      " [-1.16957138 -3.19418321]\n",
      " [-3.77378396 -2.32810805]\n",
      " [-3.19937839 -4.19803314]\n",
      " [-3.51482755 -5.39245949]]\n"
     ]
    }
   ],
   "source": [
    "observations = '33433'\n",
    "fwd = forward(hmm, observations)\n",
    "lp = fwd.lprobs.logprobs\n",
    "print(lp)\n",
    "probs = np.exp(lp)\n",
    "\n",
    "# Normalize the probabilities in case anyone implemented their code with scaling\n",
    "observed = probs/probs.sum(axis=1, keepdims=True)\n",
    "expected = np.array([[ 0.5       ,  0.5       ],\n",
    "                     [ 0.81818182,  0.18181818],\n",
    "                     [ 0.88335704,  0.11664296],\n",
    "                     [ 0.19066794,  0.80933206],\n",
    "                     [ 0.730794  ,  0.269206  ],\n",
    "                     [ 0.86733889,  0.13266111]])\n",
    "\n",
    "assert (np.linalg.norm(observed - expected, 2) < 1e-8)               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-3.11486346 -3.72045954]\n",
      " [-2.71631985 -3.0898744 ]\n",
      " [-2.40087069 -1.89544805]\n",
      " [-0.77805169 -1.41181732]\n",
      " [-0.37106368 -0.89159812]\n",
      " [ 0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "bwd = backward(hmm, observations)\n",
    "lp =  bwd.lprobs.logprobs\n",
    "probs = np.exp(lp)\n",
    "print(lp)\n",
    "\n",
    "# Normalize the probabilities in case anyone implemented their code with scaling\n",
    "observed = np.round(np.flip(probs/probs.sum(axis=1, keepdims=True), 0), 4)\n",
    "expected = np.array([[ 0.5   ,  0.5   ],\n",
    "                     [ 0.6273,  0.3727],\n",
    "                     [ 0.6533,  0.3467],\n",
    "                     [ 0.3763,  0.6237],\n",
    "                     [ 0.5923,  0.4077],\n",
    "                     [ 0.6469,  0.3531]])\n",
    "\n",
    "assert (np.linalg.norm(observed - expected, 2) < 1e-8)       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02569616 0.00405678]\n",
      " [0.00244725 0.00210351]]\n",
      "[1. 1.]\n"
     ]
    }
   ],
   "source": [
    "hmm_plus_data = HMM_plus_data(hmm, observations)\n",
    "print(np.exp(hmm_plus_data.compute_logprob_joint(1)))\n",
    "print(np.sum(np.exp(hmm_plus_data.compute_prob_conditional(1)), axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
